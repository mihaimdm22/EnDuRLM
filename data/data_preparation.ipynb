{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "data_preparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGUbUKk1y3R2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "from spacy.lang.en import English\n",
        "nlp = English()  # use directly\n",
        "\n",
        "# load nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# load text from 1st chapter\n",
        "with open('mimic_45k_random.txt', 'r') as f: \n",
        "    data = f.read()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bJtX9O5y3R8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraphs = data.split(\"\\n\\n\")\n",
        "len(paragraphs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JWwPMwDy3SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraph_sentence_list = []\n",
        "for paragraph in paragraphs:\n",
        "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
        "    paragraph = paragraph.replace(\"--\", \"\")\n",
        "    paragraph = re.sub(r'[^a-zA-Z0-9_*.,?!åäöèÅÄÖÈÉçëË]', ' ', paragraph)\n",
        "    paragraph_sentence_list.append(sent_tokenize(paragraph))\n",
        "\n",
        "len(paragraph_sentence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7riufgVZy3SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\n",
        "count = 0\n",
        "for paragraph in paragraph_sentence_list:\n",
        "    if \" \".join(paragraph).isupper():\n",
        "        with open(\"new.txt\", \"w\") as fw:\n",
        "            fw.write(text)\n",
        "        text = \"\"\n",
        "        count += 1\n",
        "        text += \"\\n\".join(paragraph)\n",
        "        text += \"\\n\\n\"\n",
        "    elif \"End of the Project Gutenberg EBook\" in \" \".join(paragraph):\n",
        "        break\n",
        "    else:\n",
        "        text += \"\\n\".join(paragraph)\n",
        "        text += \"\\n\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MFXiM6Kwy3SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences\n",
        "\n",
        "data = pd.read_csv(\"mimicmammo.csv\", usecols=[1], error_bad_lines=False)\n",
        "print(len(data))\n",
        "# for row in range(len(data)):\n",
        "#     entry = data.iloc[row][\"text\"]\n",
        "#     check = entry.split()\n",
        "#     print([word for word in check if word.isupper()])\n",
        "f = open('preprocessed' + \"mimicdemo.txt\", 'w+')\n",
        "\n",
        "i = 0\n",
        "for row in range(len(data)):\n",
        "    entry = data.iloc[row][\"text\"]\n",
        "    check = entry.split()\n",
        "#     entry = data.iloc[100][\"text\"]\n",
        "#     print(type(entry))\n",
        "#     print(\"ENTRY: \")\n",
        "#     print(entry)\n",
        "\n",
        "    text = entry\n",
        "    text = text.replace(\"_\", \"\")\n",
        "#     print(\"CHANGED5\")\n",
        "#     print(text)\n",
        "\n",
        "    text = re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '<unk>', text)\n",
        "#     print(\"CHANGED1\")\n",
        "#     print(text)\n",
        "\n",
        "    text = text.replace('\\n', '').replace('\\r', '')\n",
        "#     print(\"CHANGED0\")\n",
        "#     print(entry)\n",
        "\n",
        "    words = [\"FINAL REPORT\", \"MEDICAL CONDITION\", \"REASON FOR THIS EXAMINATION\", \"Reason:\", \"Admitting Diagnosis:\",\n",
        "            \"REASON FOR THIS EXAMINATION:\", \"PROVISIONAL FINDINGS IMPRESSION (PFI):\", \"INDICATION:\", \"FINDINGS:\", \"NOTE:\",\n",
        "            \"IMPRESSION:\"]\n",
        "    for w in range(len(words)):\n",
        "        word = words[w]\n",
        "        text = text.replace(word, \". \"+word, 1)\n",
        "\n",
        "    from nltk import tokenize\n",
        "#     print(\"TOKENIZER\")\n",
        "#     print(tokenize.sent_tokenize(text))\n",
        "\n",
        "    sentences = split_into_sentences(text)\n",
        "#     print(\"SENTENCES\")\n",
        "#     print(sentences)\n",
        "#     print(\"CHANGED0.5\")\n",
        "    for s in range(len(sentences)):\n",
        "        text = sentences[s]\n",
        "        text = re.sub('[\\W]+', ' ', text.lower())\n",
        "#         text = re.sub(\" \\d+\", \" N \", text)\n",
        "        text = re.sub(\"\\d+\", \" N \", text)\n",
        "        text = re.sub(\"v\", \"\", text)\n",
        "        text = text.replace(\"unk\", \"<unk>\")\n",
        "        \n",
        "        allow = True\n",
        "        for ind in range(len(words)):\n",
        "            elim = words[ind]\n",
        "            elim = re.sub('[\\W]+', ' ', elim.lower())\n",
        "            if text == \"\" or text == \"N x N\" or text == elim or text == elim + \"N\" or text == \"N\" or text == \"N cm\" or text == \"N mm\" or text == \"N cm x N\" or text.startswith(\"<unk> rads N\") == True:\n",
        "#             print(\"SENTENCE \" + str(s) + \" \" + text)\n",
        "                allow = False\n",
        "                break\n",
        "        if allow == True:\n",
        "            f.write(\" \" + text + \" \\n\")\n",
        "            print(\" \" + text + \" \")\n",
        "            i = i + 1\n",
        "print(i)\n",
        "f.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stYhHHQay3SM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('preprocessed' + \"mimicdemo.txt\", 'r')\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "for l in range(len(lines)):\n",
        "    print(lines[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2J7R-Lly3ST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_my_string():\n",
        "   \"\"\"Returns a string of the text\"\"\"\n",
        "   f = open(\"preprocessedmimicdemo.txt\", 'r')\n",
        "   string = str(f.read())\n",
        "   f.close()\n",
        "   return string\n",
        "\n",
        "print(get_my_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPMKvVEHy3SX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# Configure paths to your dataset files here\n",
        "DATASET_FILE = 'preprocessedmimicdemo.txt'\n",
        "FILE_TRAIN = 'ppmimictrain.txt'\n",
        "FILE_VALID = 'ppmimicvalidation.txt'\n",
        "FILE_TESTS = 'ppmimictest.txt'\n",
        "\n",
        "# Set to true if you want to copy first line from main\n",
        "# file into each split (like CSV header)\n",
        "IS_CSV = False\n",
        "\n",
        "# Make sure it adds to 100, no error checking below\n",
        "PERCENT_TRAIN = 70\n",
        "PERCENT_VALID = 15\n",
        "PERCENT_TESTS = 15\n",
        "\n",
        "data = [l for l in open(DATASET_FILE, 'r')]\n",
        "\n",
        "train_file = open(FILE_TRAIN, 'w+')\n",
        "valid_file = open(FILE_VALID, 'w+')\n",
        "tests_file = open(FILE_TESTS, 'w+')\n",
        "\n",
        "if IS_CSV:\n",
        "    train_file.write(data[0])\n",
        "    valid_file.write(data[0])\n",
        "    tests_file.write(data[0])\n",
        "    data = data[1:len(data)]\n",
        "\n",
        "num_of_data = len(data)\n",
        "num_train = int((PERCENT_TRAIN/100.0)*num_of_data)\n",
        "num_valid = int((PERCENT_VALID/100.0)*num_of_data)\n",
        "num_tests = int((PERCENT_TESTS/100.0)*num_of_data)\n",
        "\n",
        "data_fractions = [num_train, num_valid, num_tests]\n",
        "split_data = [[],[],[]]\n",
        "\n",
        "rand_data_ind = 0\n",
        "\n",
        "for split_ind, fraction in enumerate(data_fractions):\n",
        "    for i in range(fraction):\n",
        "        rand_data_ind = random.randint(0, len(data)-1)\n",
        "        split_data[split_ind].append(data[rand_data_ind])\n",
        "        data.pop(rand_data_ind)\n",
        "\n",
        "for l in split_data[0]:\n",
        "    train_file.write(l)\n",
        "    \n",
        "for l in split_data[1]:\n",
        "    valid_file.write(l)\n",
        "    \n",
        "for l in split_data[2]:\n",
        "    tests_file.write(l)\n",
        "    \n",
        "train_file.close()\n",
        "valid_file.close()\n",
        "tests_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}