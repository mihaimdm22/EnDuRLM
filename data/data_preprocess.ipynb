{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "data_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "529p9qe5yV3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mxnet as mx\n",
        "from mxnet import gluon, nd\n",
        "import gluonnlp as nlp\n",
        "\n",
        "train_dataset = nlp.data.TextLineDataset('mimicdemo.train.txt', encoding='utf8')\n",
        "test_dataset = nlp.data.TextLineDataset('mimicdemo.test.txt', encoding='utf8')\n",
        "\n",
        "print('#training samples={:d}, #testing samples={:d}'.format(len(train_dataset), len(test_dataset)))\n",
        "print(train_dataset[0])\n",
        "\n",
        "import nltk\n",
        "moses_tokenizer = nlp.data.SacreMosesTokenizer()\n",
        "\n",
        "spacy_tokenizer = nlp.data.SpacyTokenizer()\n",
        "\n",
        "dataset = nlp.data.CorpusDataset(\n",
        "    'mimicdemo.train.txt',\n",
        "#     sample_splitter=nltk.tokenize.sent_tokenize,\n",
        "    tokenizer=spacy_tokenizer\n",
        "#     flatten=False,\n",
        "#     eos='<eos>'\n",
        "    )\n",
        "\n",
        "print('# sentences:', len(dataset))\n",
        "for sentence in dataset[:50]:\n",
        "    print('# tokens:', len(sentence), sentence[:100])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX-Bg-C2yV3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "train_dataset = nlp.data.TextLineDataset('mimicdemo.train.txt', encoding='utf8')\n",
        "\n",
        "for index in range(0, len(train_dataset) - 1):\n",
        "    sentence = train_dataset[index]\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', sentence, re.UNICODE)\n",
        "    sentence = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    print(sentence)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHoyo2X9yV3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "def cleantext(filename):\n",
        "    textlinedataset = nlp.data.TextLineDataset(filename + '.txt', encoding='utf8')\n",
        "    file = open(filename + '.clean.txt','w+') \n",
        "    for i in range(0, len(textlinedataset)):\n",
        "        line = textlinedataset[i]\n",
        "#         line = remove_between_square_brackets(textlinedataset[i])\n",
        "#         line = replace_contractions(line)\n",
        "#         print(line)\n",
        "        file.write(line)\n",
        "    file.close()    \n",
        "\n",
        "cleantext('mimicdemo.train')\n",
        "\n",
        "dataset = nlp.data.CorpusDataset(\n",
        "    'mimicdemo.train.clean.txt',\n",
        "#     sample_splitter=nltk.tokenize.sent_tokenize,\n",
        "    tokenizer=spacy_tokenizer\n",
        "#     flatten=False,\n",
        "#     eos='<eos>'\n",
        "    )\n",
        "\n",
        "print('# sentences:', len(dataset))\n",
        "for sentence in dataset[:50]:\n",
        "    print('# tokens:', len(sentence), sentence[:100])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwbxHh9AyV3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textlinedataset = nlp.data.TextLineDataset('mimicdemo.train.clean.txt', encoding='utf8')\n",
        "# print(textlinedataset[0])\n",
        "\n",
        "from spacy.lang.en import English \n",
        "import re\n",
        "\n",
        "\n",
        "nlpp = English()\n",
        "nlpp.add_pipe(nlpp.create_pipe('sentencizer'))\n",
        "doc = nlpp(textlinedataset[0])\n",
        "file = open('mimicdemo.train.final.txt','w+') \n",
        "for sent in doc.sents:\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent.text, re.UNICODE)\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "\n",
        "\n",
        "    sent = re.sub(r'[^\\w\\s]', '', sent, re.UNICODE)\n",
        "    sent = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', sent)\n",
        "    sent = sent.lower()\n",
        "    file.write(sent + \"\\n\")\n",
        "    print(sent)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbl2MJcoyV3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_tokenizer = nlp.data.SpacyTokenizer()\n",
        "\n",
        "dataset = nlp.data.CorpusDataset(\n",
        "    'mimicdemo.train.final.txt',\n",
        "#     sample_splitter=nltk.tokenize.sent_tokenize,\n",
        "    tokenizer=spacy_tokenizer\n",
        "#     flatten=False,\n",
        "#     eos='<eos>'\n",
        "    )\n",
        "\n",
        "print('# sentences:', len(dataset))\n",
        "for sentence in dataset[:50]:\n",
        "    print('# tokens:', len(sentence), sentence[:100])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Y5nOekyV3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "import math\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import mxnet as mx\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "context = mx.cpu()  # Enable this to run on CPU\n",
        "# context = mx.gpu(0)  # Enable this to run on GPU\n",
        "\n",
        "text8 = dataset\n",
        "counter = nlp.data.count_tokens(itertools.chain.from_iterable(text8))\n",
        "vocab = nlp.Vocab(counter, unknown_token=None, padding_token=None,\n",
        "                  bos_token=None, eos_token=None, min_freq=10)\n",
        "idx_to_counts = [counter[w] for w in vocab.idx_to_token]\n",
        "\n",
        "def code(sentence):\n",
        "    return [vocab[token] for token in sentence if token in vocab]\n",
        "\n",
        "text8 = text8.transform(code, lazy=False)\n",
        "\n",
        "print('# sentences:', len(text8))\n",
        "for sentence in text8[:3]:\n",
        "    print('# tokens:', len(sentence), sentence[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHT0k8y_yV3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data import transform_data_fasttext\n",
        "batch_size=4096\n",
        "data = nlp.data.SimpleDataStream([text8])  # input is a stream of datasets, here just 1. Allows scaling to larger corpora that don't fit in memory\n",
        "data, batchify_fn, subword_function = transform_data_fasttext(\n",
        "    data, vocab, idx_to_counts, cbow=False, ngrams=[3,4,5,6], ngram_buckets=100000, batch_size=batch_size, window_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p20_FLKEyV3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = data.transform(batchify_fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IfTlTFyV3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_to_subwordidxs = subword_function(vocab.idx_to_token)\n",
        "for word, subwords in zip(vocab.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
        "    print('<'+word+'>', subwords, sep = '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru7lBGP0yV3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from model import SG as SkipGramNet\n",
        "\n",
        "emsize = 300\n",
        "num_negatives = 5\n",
        "\n",
        "negatives_weights = mx.nd.array(idx_to_counts)\n",
        "embedding = SkipGramNet(\n",
        "    vocab.token_to_idx, emsize, batch_size, negatives_weights, subword_function, num_negatives=5, smoothing=0.75)\n",
        "embedding.initialize(ctx=context)\n",
        "embedding.hybridize()\n",
        "trainer = mx.gluon.Trainer(embedding.collect_params(), 'adagrad', dict(learning_rate=0.05))\n",
        "\n",
        "print(embedding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpmHkhktyV3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(SkipGramNet.hybrid_forward.__doc__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr36FUciyV3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm_vecs_by_row(x):\n",
        "    return x / (mx.nd.sum(x * x, axis=1) + 1e-10).sqrt().reshape((-1, 1))\n",
        "\n",
        "\n",
        "def get_k_closest_tokens(vocab, embedding, k, word):\n",
        "    word_vec = norm_vecs_by_row(embedding[[word]])\n",
        "    vocab_vecs = norm_vecs_by_row(embedding[vocab.idx_to_token])\n",
        "    dot_prod = mx.nd.dot(vocab_vecs, word_vec.T)\n",
        "    indices = mx.nd.topk(\n",
        "        dot_prod.reshape((len(vocab.idx_to_token), )),\n",
        "        k=k + 1,\n",
        "        ret_typ='indices')\n",
        "    indices = [int(i.asscalar()) for i in indices]\n",
        "    result = [vocab.idx_to_token[i] for i in indices[1:]]\n",
        "    print('closest tokens to \"%s\": %s' % (word, \", \".join(result)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64sZCCB6yV3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_token = \"radiography\"\n",
        "get_k_closest_tokens(vocab, embedding, 10, example_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygnYkFCnyV3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_interval = 500\n",
        "\n",
        "def train_embedding(num_epochs):\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "        l_avg = 0\n",
        "        log_wc = 0\n",
        "\n",
        "        print('Beginnign epoch %d and resampling data.' % epoch)\n",
        "        for i, batch in enumerate(batches):\n",
        "            batch = [array.as_in_context(context) for array in batch]\n",
        "            with mx.autograd.record():\n",
        "                l = embedding(*batch)\n",
        "            l.backward()\n",
        "            trainer.step(1)\n",
        "\n",
        "            l_avg += l.mean()\n",
        "            log_wc += l.shape[0]\n",
        "            if i % log_interval == 0:\n",
        "                mx.nd.waitall()\n",
        "                wps = log_wc / (time.time() - start_time)\n",
        "                l_avg = l_avg.asscalar() / log_interval\n",
        "                print('epoch %d, iteration %d, loss %.2f, throughput=%.2fK wps'\n",
        "                      % (epoch, i, l_avg, wps / 1000))\n",
        "                start_time = time.time()\n",
        "                log_wc = 0\n",
        "                l_avg = 0\n",
        "\n",
        "        get_k_closest_tokens(vocab, embedding, 10, example_token)\n",
        "        print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "81kdOjc-yV32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_embedding(num_epochs=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}